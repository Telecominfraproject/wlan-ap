From a3f135a60d6ccf3e3f569836b3cde0ab5e489e60 Mon Sep 17 00:00:00 2001
From: Tamizh Chelvam Raja <quic_tamizhr@quicinc.com>
Date: Sun, 7 Jan 2024 13:35:42 +0530
Subject: [PATCH] wifi: ath12k: Optimize the Tx data path by remove the used
 list processing

In order to avoid taking spin lock for each desc in tx completion
to add desc in used list and add it to free list. Keep the desc
in a local lists and free list at single shot.

Signed-off-by: Tamizh Chelvam Raja <quic_tamizhr@quicinc.com>
---
 drivers/net/wireless/ath/ath12k/dp.c    | 82 ++++++++++++++-----------
 drivers/net/wireless/ath/ath12k/dp.h    |  3 +-
 drivers/net/wireless/ath/ath12k/dp_tx.c | 18 +++++-
 3 files changed, 64 insertions(+), 39 deletions(-)

--- a/drivers/net/wireless/ath/ath12k/dp.c
+++ b/drivers/net/wireless/ath/ath12k/dp.c
@@ -1623,10 +1623,11 @@ void ath12k_dp_vdev_tx_attach(struct ath
 void ath12k_dp_umac_txrx_desc_cleanup(struct ath12k_base *ab)
 {
 	struct ath12k_rx_desc_info *desc_info;
-	struct ath12k_tx_desc_info *tx_desc_info, *tmp1;
+	struct ath12k_tx_desc_info *tx_desc_info;
 	struct ath12k_dp *dp = &ab->dp;
 	struct sk_buff *skb;
-	int i, j;
+	int i, j, k;
+	u32  tx_spt_page;
 
 	/* RX Descriptor cleanup */
 	spin_lock_bh(&dp->rx_desc_lock);
@@ -1659,25 +1660,31 @@ void ath12k_dp_umac_txrx_desc_cleanup(st
 	for (i = 0; i < ATH12K_HW_MAX_QUEUES; i++) {
 		spin_lock_bh(&dp->tx_desc_lock[i]);
 
-		list_for_each_entry_safe(tx_desc_info, tmp1, &dp->tx_desc_used_list[i],
-					 list) {
-			list_move_tail(&tx_desc_info->list, &ab->dp.tx_desc_free_list);
-			skb = tx_desc_info->skb;
-			tx_desc_info->skb = NULL;
-
-			if (!skb)
-				continue;
-
-			dma_unmap_single(ab->dev, ATH12K_SKB_CB(skb)->paddr,
-					 skb->len, DMA_TO_DEVICE);
-
-			if (tx_desc_info->skb_ext_desc) {
-				dma_unmap_single(ab->dev, ATH12K_SKB_CB(skb)->paddr_ext_desc,
-						 tx_desc_info->skb_ext_desc->len, DMA_TO_DEVICE);
-				dev_kfree_skb_any(tx_desc_info->skb_ext_desc);
+		for (j = 0; j < ATH12K_TX_SPT_PAGES_PER_POOL; j++) {
+			tx_spt_page = j + i * ATH12K_TX_SPT_PAGES_PER_POOL;
+			tx_desc_info = dp->spt_info->txbaddr[tx_spt_page];
+
+			for (k = 0; k < ATH12K_MAX_SPT_ENTRIES; k++) {
+				if (!tx_desc_info[k].in_use)
+					continue;
+
+				skb = tx_desc_info[k].skb;
+				if (!skb)
+					continue;
+
+				tx_desc_info[k].skb = NULL;
+				tx_desc_info[k].skb_ext_desc = NULL;
+				tx_desc_info[k].in_use = false;
+				list_add_tail(&tx_desc_info[k].list, &dp->tx_desc_free_list[i]);
+				dma_unmap_single(ab->dev, ATH12K_SKB_CB(skb)->paddr,
+						 skb->len, DMA_TO_DEVICE);
+				if (tx_desc_info[k].skb_ext_desc) {
+					dma_unmap_single(ab->dev, ATH12K_SKB_CB(skb)->paddr_ext_desc,
+							 tx_desc_info[k].skb_ext_desc->len, DMA_TO_DEVICE);
+					dev_kfree_skb_any(tx_desc_info[k].skb_ext_desc);
+				}
+				dev_kfree_skb_any(skb);
 			}
-
-			dev_kfree_skb_any(skb);
 		}
 
 		spin_unlock_bh(&dp->tx_desc_lock[i]);
@@ -1687,10 +1694,10 @@ void ath12k_dp_umac_txrx_desc_cleanup(st
 static void ath12k_dp_cc_cleanup(struct ath12k_base *ab)
 {
 	struct ath12k_rx_desc_info *desc_info;
-	struct ath12k_tx_desc_info *tx_desc_info, *tmp1;
+	struct ath12k_tx_desc_info *tx_desc_info;
 	struct ath12k_dp *dp = &ab->dp;
 	struct sk_buff *skb;
-	int i, j;
+	int i, j, k;
 	u32  pool_id, tx_spt_page;
 
 	if (!dp->spt_info)
@@ -1731,24 +1738,29 @@ static void ath12k_dp_cc_cleanup(struct
 	for (i = 0; i < ATH12K_HW_MAX_QUEUES; i++) {
 		spin_lock_bh(&dp->tx_desc_lock[i]);
 
-		list_for_each_entry_safe(tx_desc_info, tmp1, &dp->tx_desc_used_list[i],
-					 list) {
-			list_del(&tx_desc_info->list);
-			skb = tx_desc_info->skb;
-
-			if (!skb)
-				continue;
-
-			dma_unmap_single(ab->dev, ATH12K_SKB_CB(skb)->paddr,
-					 skb->len, DMA_TO_DEVICE);
-
-			if (tx_desc_info->skb_ext_desc) {
-				dma_unmap_single(ab->dev, ATH12K_SKB_CB(skb)->paddr_ext_desc,
-						 tx_desc_info->skb_ext_desc->len, DMA_TO_DEVICE);
-				dev_kfree_skb_any(tx_desc_info->skb_ext_desc);
+		for (j = 0; j < ATH12K_TX_SPT_PAGES_PER_POOL; j++) {
+			tx_spt_page = j + i * ATH12K_TX_SPT_PAGES_PER_POOL;
+			tx_desc_info = dp->spt_info->txbaddr[tx_spt_page];
+			for (k = 0; k < ATH12K_MAX_SPT_ENTRIES; k++) {
+				if (!tx_desc_info[k].in_use)
+					continue;
+
+				skb = tx_desc_info[k].skb;
+
+				if (!skb)
+					continue;
+
+				tx_desc_info[k].skb = NULL;
+				dma_unmap_single(ab->dev, ATH12K_SKB_CB(skb)->paddr,
+						 skb->len, DMA_TO_DEVICE);
+
+				if (tx_desc_info[k].skb_ext_desc) {
+					dma_unmap_single(ab->dev, ATH12K_SKB_CB(skb)->paddr_ext_desc,
+							 tx_desc_info[k].skb_ext_desc->len, DMA_TO_DEVICE);
+					dev_kfree_skb_any(tx_desc_info[k].skb_ext_desc);
+				}
+				dev_kfree_skb_any(skb);
 			}
-
-			dev_kfree_skb_any(skb);
 		}
 
 		spin_unlock_bh(&dp->tx_desc_lock[i]);
--- a/drivers/net/wireless/ath/ath12k/dp.h
+++ b/drivers/net/wireless/ath/ath12k/dp.h
@@ -336,7 +336,8 @@ struct ath12k_tx_desc_info {
 	struct sk_buff *skb;
 	struct sk_buff *skb_ext_desc;
 	u32 desc_id; /* Cookie */
-	u8 mac_id;
+	u8 mac_id : 5,
+	   in_use : 1;
 	u8 pool_id;
 	u8 recycler_fast_xmit;
 	ktime_t timestamp;
--- a/drivers/net/wireless/ath/ath12k/dp_tx.c
+++ b/drivers/net/wireless/ath/ath12k/dp_tx.c
@@ -175,7 +175,8 @@ static void ath12k_dp_tx_release_txbuf(s
 	tx_desc->skb = NULL;
 	tx_desc->skb_ext_desc = NULL;
 	spin_lock_bh(&dp->tx_desc_lock[ring_id]);
-	list_move_tail(&tx_desc->list, &dp->tx_desc_free_list[ring_id]);
+	tx_desc->in_use = false;
+	list_add_tail(&tx_desc->list, &dp->tx_desc_free_list[ring_id]);
 	spin_unlock_bh(&dp->tx_desc_lock[ring_id]);
 }
 
@@ -196,7 +197,8 @@ struct ath12k_tx_desc_info *ath12k_dp_tx
 	}
 
 	prefetch(desc);
-	list_move_tail(&desc->list, &dp->tx_desc_used_list[pool_id]);
+	list_del(&desc->list);
+	desc->in_use = true;
 	spin_unlock_bh(&dp->tx_desc_lock[pool_id]);
 
 	return desc;
@@ -1526,6 +1528,7 @@ int ath12k_dp_tx_completion_handler(stru
 	int hal_ring_id = dp->tx_ring[ring_id].tcl_comp_ring.ring_id;
 	struct hal_srng *status_ring = &ab->hal.srng_list[hal_ring_id];
 	struct ath12k_tx_desc_info *tx_desc = NULL;
+	struct list_head desc_free_list, *cur;
 	struct sk_buff *msdu, *skb_ext_desc;
 	struct ath12k_skb_cb *skb_cb;
 	struct dp_tx_ring *tx_ring = &dp->tx_ring[ring_id];
@@ -1544,6 +1547,8 @@ int ath12k_dp_tx_completion_handler(stru
 	int recycler_fast_xmit;
 
 
+	INIT_LIST_HEAD(&desc_free_list);
+
 	ath12k_hal_srng_access_dst_ring_begin_nolock(ab, status_ring);
 
 	valid_entries = ath12k_hal_srng_dst_num_free(ab, status_ring, false);
@@ -1589,7 +1594,7 @@ int ath12k_dp_tx_completion_handler(stru
 		/* Release descriptor as soon as extracting necessary info
 		 * to reduce contention
 		 */
-		ath12k_dp_tx_release_txbuf(dp, tx_desc, tx_desc->pool_id);
+		list_add_tail(&tx_desc->list, &desc_free_list);
 
 		if (unlikely(!msdu)) {
 			ab->soc_stats.null_tx_complete[tx_ring->tcl_data_ring_id]++;
@@ -1679,6 +1684,17 @@ int ath12k_dp_tx_completion_handler(stru
 						   tx_ring->tcl_data_ring_id, timestamp);
 		}
 	}
+
+	spin_lock_bh(&dp->tx_desc_lock[ring_id]);
+	list_for_each(cur, &desc_free_list) {
+		tx_desc = list_entry(cur, struct ath12k_tx_desc_info, list);
+		tx_desc->skb = NULL;
+		tx_desc->skb_ext_desc = NULL;
+		tx_desc->in_use = false;
+	}
+	list_splice_tail(&desc_free_list, &dp->tx_desc_free_list[ring_id]);
+	spin_unlock_bh(&dp->tx_desc_lock[ring_id]);
+
 	ath12k_hal_srng_access_dst_ring_end_nolock(status_ring);
 	dev_kfree_skb_list_fast(&free_list_head);
 	return (orig_budget - budget);
--- a/drivers/net/wireless/ath/ath12k/bondif.c
+++ b/drivers/net/wireless/ath/ath12k/bondif.c
@@ -493,7 +493,8 @@ static void ath12k_dp_tx_release_txbuf(s
 	tx_desc->skb = NULL;
 	tx_desc->skb_ext_desc = NULL;
 	spin_lock_bh(&dp->tx_desc_lock[ring_id]);
-	list_move_tail(&tx_desc->list, &dp->tx_desc_free_list[ring_id]);
+	tx_desc->in_use = false;
+	list_add_tail(&tx_desc->list, &dp->tx_desc_free_list[ring_id]);
 	spin_unlock_bh(&dp->tx_desc_lock[ring_id]);
 }
 
@@ -514,7 +515,8 @@ struct ath12k_tx_desc_info *ath12k_dp_tx
 	}
 
 	prefetch(desc);
-	list_move_tail(&desc->list, &dp->tx_desc_used_list[pool_id]);
+	list_del(&desc->list);
+	desc->in_use = true;
 	spin_unlock_bh(&dp->tx_desc_lock[pool_id]);
 
 	return desc;
