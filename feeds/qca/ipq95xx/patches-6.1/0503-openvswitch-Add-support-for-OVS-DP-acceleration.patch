From 9b9d11a9d0c4f8c91bdefc1ba95a1bca57833eaa Mon Sep 17 00:00:00 2001
From: Ratheesh Kannoth <rkannoth@codeaurora.org>
Date: Tue, 30 Jun 2020 11:20:02 +0530
Subject: [PATCH] openvswitch: Add support for OVS DP acceleration

 This patch will allow inline datapath accelerator
 to register for bridge notifications and packet
 processing callbacks

Change-Id: If69faec1abafa7a6cc54835a60c16bb4bd789275
Signed-off-by: Pavithra R <quic_pavir@quicinc.com>
---
 include/uapi/linux/openvswitch.h |   2 +-
 net/openvswitch/datapath.c       | 291 ++++++++++++++++++++++++++++++-
 net/openvswitch/datapath.h       |  41 +++++
 3 files changed, 332 insertions(+), 2 deletions(-)

diff --git a/include/uapi/linux/openvswitch.h b/include/uapi/linux/openvswitch.h
index 94066f87e9ee..73677d9edd1f 100644
--- a/include/uapi/linux/openvswitch.h
+++ b/include/uapi/linux/openvswitch.h
@@ -277,6 +277,7 @@ enum ovs_vport_attr {
 	OVS_VPORT_ATTR_PAD,
 	OVS_VPORT_ATTR_IFINDEX,
 	OVS_VPORT_ATTR_NETNSID,
+	OVS_VPORT_ATTR_MASTER,	/* Master device for this vport, device name */
 	__OVS_VPORT_ATTR_MAX
 };
 
@@ -290,7 +291,6 @@ enum {
 
 #define OVS_VXLAN_EXT_MAX (__OVS_VXLAN_EXT_MAX - 1)
 
-
 /* OVS_VPORT_ATTR_OPTIONS attributes for tunnels.
  */
 enum {
diff --git a/net/openvswitch/datapath.c b/net/openvswitch/datapath.c
index 5920fdca1287..e43916893b14 100644
--- a/net/openvswitch/datapath.c
+++ b/net/openvswitch/datapath.c
@@ -49,6 +49,7 @@
 #include "vport-netdev.h"
 
 unsigned int ovs_net_id __read_mostly;
+static struct ovs_accel_callback *ovs_accel_cb;
 
 static struct genl_family dp_packet_genl_family;
 static struct genl_family dp_flow_genl_family;
@@ -220,6 +221,126 @@ void ovs_dp_detach_port(struct vport *p)
 	ovs_vport_del(p);
 }
 
+/* Notify datapath add event to acceleration callback */
+static void ovs_dp_add_notify(struct datapath *dp, struct vport *vp)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	rcu_read_lock();
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_add)
+		ovs_cb->ovs_accel_dp_add((void *)dp, vp->dev);
+	rcu_read_unlock();
+}
+
+/* Notify datapath delete event to acceleration callback */
+static void ovs_dp_del_notify(struct datapath *dp, struct vport *vp)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	rcu_read_lock();
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_del)
+		ovs_cb->ovs_accel_dp_del((void *)dp, vp->dev);
+	rcu_read_unlock();
+}
+
+/* Notify datapath port add event to acceleration callback */
+static void ovs_dp_port_add_notify(struct datapath *dp, struct vport *vp,
+				   struct nlattr **a)
+{
+	struct ovs_accel_callback *ovs_cb;
+	const char *master = NULL;
+
+	if (a[OVS_VPORT_ATTR_MASTER])
+		master = nla_data(a[OVS_VPORT_ATTR_MASTER]);
+
+	rcu_read_lock();
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_port_add)
+		ovs_cb->ovs_accel_dp_port_add((void *)dp, (void *)vp,
+					      vp->port_no, vp->ops->type,
+					      master, vp->dev);
+	rcu_read_unlock();
+}
+
+/* Notify datapath port delete event to acceleration callback */
+static void ovs_dp_port_del_notify(struct datapath *dp, struct vport *vp)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	rcu_read_lock();
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_port_del)
+		ovs_cb->ovs_accel_dp_port_del((void *)dp, (void *)vp, vp->dev);
+	rcu_read_unlock();
+}
+
+/* Notify datapath flow add event to acceleration callback */
+static void ovs_dp_flow_add_notify(struct datapath *dp, struct sw_flow *sf)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	rcu_read_lock();
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_flow_add)
+		ovs_cb->ovs_accel_dp_flow_add((void *)dp, sf);
+	rcu_read_unlock();
+}
+
+/* Notify datapath flow delete event to acceleration callback */
+static void ovs_dp_flow_del_notify(struct datapath *dp, struct sw_flow *sf)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	rcu_read_lock();
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_flow_del)
+		ovs_cb->ovs_accel_dp_flow_del((void *)dp, sf);
+	rcu_read_unlock();
+}
+
+/* Notify datapath flow table flush event to acceleration callback */
+static void ovs_dp_flow_tbl_flush_notify(struct datapath *dp)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	rcu_read_lock();
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_flow_tbl_flush)
+		ovs_cb->ovs_accel_dp_flow_tbl_flush((void *)dp);
+	rcu_read_unlock();
+}
+
+/* Notify datapath flow set/change event to acceleration callback */
+static void ovs_dp_flow_set_notify(struct datapath *dp, struct sw_flow *sf,
+				   struct sw_flow_actions *new_sfa)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	rcu_read_lock();
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_flow_set)
+		ovs_cb->ovs_accel_dp_flow_set((void *)dp, sf, new_sfa);
+	rcu_read_unlock();
+}
+
+/* Forward datapath packet to acceleration callback
+ * Must be called with rcu_read_lock.
+ */
+static void ovs_dp_pkt_process_notify(struct datapath *dp, struct sk_buff *skb,
+				      struct sw_flow_key *key, struct sw_flow *sf,
+		struct sw_flow_actions *sfa)
+{
+	struct ovs_accel_callback *ovs_cb;
+
+	WARN_ON(!rcu_read_lock_held());
+
+	ovs_cb = rcu_dereference(ovs_accel_cb);
+	if (ovs_cb && ovs_cb->ovs_accel_dp_pkt_process)
+		ovs_cb->ovs_accel_dp_pkt_process((void *)dp, skb, key, sf, sfa);
+}
+
 /* Must be called with rcu_read_lock. */
 void ovs_dp_process_packet(struct sk_buff *skb, struct sw_flow_key *key)
 {
@@ -269,6 +390,7 @@ void ovs_dp_process_packet(struct sk_buff *skb, struct sw_flow_key *key)
 
 	ovs_flow_stats_update(flow, key->tp.flags, skb);
 	sf_acts = rcu_dereference(flow->sf_acts);
+	ovs_dp_pkt_process_notify(dp, skb, key, flow, sf_acts);
 	error = ovs_execute_actions(dp, skb, sf_acts, key);
 	if (unlikely(error))
 		net_dbg_ratelimited("ovs: action execution error on datapath %s: %d\n",
@@ -1033,6 +1155,7 @@ static int ovs_flow_cmd_new(struct sk_buff *skb, struct genl_info *info)
 			goto err_unlock_ovs;
 		}
 
+		ovs_dp_flow_add_notify(dp, new_flow);
 		if (unlikely(reply)) {
 			error = ovs_flow_cmd_fill_info(new_flow,
 						       ovs_header->dp_ifindex,
@@ -1245,6 +1368,7 @@ static int ovs_flow_cmd_set(struct sk_buff *skb, struct genl_info *info)
 	if (likely(acts)) {
 		old_acts = ovsl_dereference(flow->sf_acts);
 		rcu_assign_pointer(flow->sf_acts, acts);
+		ovs_dp_flow_set_notify(dp, flow, old_acts);
 
 		if (unlikely(reply)) {
 			error = ovs_flow_cmd_fill_info(flow,
@@ -1380,6 +1504,7 @@ static int ovs_flow_cmd_del(struct sk_buff *skb, struct genl_info *info)
 	}
 
 	if (unlikely(!a[OVS_FLOW_ATTR_KEY] && !ufid_present)) {
+		ovs_dp_flow_tbl_flush_notify(dp);
 		err = ovs_flow_tbl_flush(&dp->table);
 		goto unlock;
 	}
@@ -1393,6 +1518,7 @@ static int ovs_flow_cmd_del(struct sk_buff *skb, struct genl_info *info)
 		goto unlock;
 	}
 
+	ovs_dp_flow_del_notify(dp, flow);
 	ovs_flow_tbl_remove(&dp->table, flow);
 	ovs_unlock();
 
@@ -1839,6 +1965,7 @@ static int ovs_dp_cmd_new(struct sk_buff *skb, struct genl_info *info)
 
 	ovs_net = net_generic(ovs_dp_get_net(dp), ovs_net_id);
 	list_add_tail_rcu(&dp->list_node, &ovs_net->dps);
+	ovs_dp_add_notify(dp, vport);
 
 	ovs_unlock();
 
@@ -1882,6 +2009,7 @@ static void __dp_destroy(struct datapath *dp)
 				ovs_dp_detach_port(vport);
 	}
 
+	ovs_dp_del_notify(dp, ovs_vport_ovsl(dp, OVSP_LOCAL));
 	list_del_rcu(&dp->list_node);
 
 	/* OVSP_LOCAL is datapath internal port. We need to make sure that
@@ -2286,6 +2414,7 @@ static int ovs_vport_cmd_new(struct sk_buff *skb, struct genl_info *info)
 		goto exit_unlock_free;
 	}
 
+	ovs_dp_port_add_notify(dp, vport, a);
 	err = ovs_vport_cmd_fill_info(vport, reply, genl_info_net(info),
 				      info->snd_portid, info->snd_seq, 0,
 				      OVS_VPORT_CMD_NEW, GFP_KERNEL);
@@ -2338,7 +2467,6 @@ static int ovs_vport_cmd_set(struct sk_buff *skb, struct genl_info *info)
 			goto exit_unlock_free;
 	}
 
-
 	if (a[OVS_VPORT_ATTR_UPCALL_PID]) {
 		struct nlattr *ids = a[OVS_VPORT_ATTR_UPCALL_PID];
 
@@ -2387,6 +2515,7 @@ static int ovs_vport_cmd_del(struct sk_buff *skb, struct genl_info *info)
 		goto exit_unlock_free;
 	}
 
+	ovs_dp_port_del_notify(vport->dp, vport);
 	err = ovs_vport_cmd_fill_info(vport, reply, genl_info_net(info),
 				      info->snd_portid, info->snd_seq, 0,
 				      OVS_VPORT_CMD_DEL, GFP_KERNEL);
@@ -2417,6 +2546,166 @@ static int ovs_vport_cmd_del(struct sk_buff *skb, struct genl_info *info)
 	return err;
 }
 
+/* Register OVS datapath accelerator */
+int ovs_register_accelerator(struct ovs_accel_callback *oac)
+{
+	ovs_lock();
+
+	if (unlikely(rcu_access_pointer(ovs_accel_cb))) {
+		ovs_unlock();
+		return -EEXIST;
+	}
+
+	rcu_assign_pointer(ovs_accel_cb, oac);
+	ovs_unlock();
+	return 0;
+}
+EXPORT_SYMBOL(ovs_register_accelerator);
+
+/* Unregister OVS datapath accelerator */
+void ovs_unregister_accelerator(struct ovs_accel_callback *oac)
+{
+	ovs_lock();
+	rcu_assign_pointer(ovs_accel_cb, NULL);
+	ovs_unlock();
+}
+EXPORT_SYMBOL(ovs_unregister_accelerator);
+
+/* Find datapath flow rule using the key*/
+struct sw_flow *ovs_accel_flow_find(void *dp_inst, struct sw_flow_key *key)
+{
+	struct datapath *dp = dp_inst;
+	struct sw_flow *flow;
+
+	rcu_read_lock();
+	flow = ovs_flow_tbl_lookup(&dp->table, key);
+	rcu_read_unlock();
+
+	return flow;
+}
+EXPORT_SYMBOL(ovs_accel_flow_find);
+
+/* Update flow rule statistics */
+int ovs_accel_flow_stats_update(void *dp_inst, void *out_vport,
+				 struct sw_flow_key *key, int pkts, int bytes)
+{
+	struct datapath *dp = dp_inst;
+	struct sw_flow_stats *stats;
+	struct sw_flow *flow;
+	struct dp_stats_percpu *dp_stats;
+	int node = numa_node_id();
+	u64 *stats_counter;
+	u32 n_mask_hit;
+	u32  n_cache_hit;
+
+	rcu_read_lock();
+	flow = ovs_flow_tbl_lookup_stats(&dp->table, key, 0,
+					&n_mask_hit, &n_cache_hit);
+	if (!flow) {
+		rcu_read_unlock();
+		return -EINVAL;
+	}
+
+	/* Update node specific statistics, if memory is not allocated
+	 * for this node then update in 0 node
+	 */
+	stats = rcu_dereference(flow->stats[node]);
+	if (unlikely(!stats))
+		stats = rcu_dereference(flow->stats[0]);
+
+	rcu_read_unlock();
+
+	spin_lock(&stats->lock);
+	stats->used = jiffies;
+	stats->packet_count += pkts;
+	stats->byte_count += bytes;
+
+	/* Update datapath statistics, only hit count should be updated here,
+	 * miss count is taken care by datapath.
+	 * n_mask_hit and stats_counter are updated per packet, whereas
+	 * stats_counter will match the number of packets processed in datapath
+	 * n_mask_hit is updated number of packets times the total masks that
+	 * are processed.  Datapath flows are now accelerated and this API is
+	 * called to update flow statistics, datpath statistics should use
+	 * number of packets.
+	 */
+	dp_stats = this_cpu_ptr(dp->stats_percpu);
+	stats_counter = &dp_stats->n_hit;
+
+	u64_stats_update_begin(&dp_stats->syncp);
+	(*stats_counter) += pkts;
+	dp_stats->n_mask_hit += n_mask_hit * pkts;
+	dp_stats->n_cache_hit += n_cache_hit * pkts;
+	u64_stats_update_end(&dp_stats->syncp);
+
+	spin_unlock(&stats->lock);
+	return 0;
+}
+EXPORT_SYMBOL(ovs_accel_flow_stats_update);
+
+/* Find netdev using vport number */
+struct net_device *ovs_accel_dev_find(void *dp_inst, int vport_no)
+{
+	struct datapath *dp = dp_inst;
+	struct net_device *dev;
+	struct vport *vport;
+
+	rcu_read_lock();
+
+	vport = ovs_vport_rcu(dp, vport_no);
+	if (!vport) {
+		rcu_read_unlock();
+		return NULL;
+	}
+
+	dev = vport->dev;
+	rcu_read_unlock();
+	return dev;
+}
+EXPORT_SYMBOL(ovs_accel_dev_find);
+
+/* Find egress interface using key and skb */
+struct net_device *ovs_accel_egress_dev_find(void *dp_inst,
+					     struct sw_flow_key *key,
+					     struct sk_buff *skb)
+{
+	struct datapath *dp = dp_inst;
+	struct sw_flow *flow;
+	struct sw_flow_actions *sf_acts;
+	struct net_device *dev;
+	const struct nlattr *a;
+	int rem;
+
+	rcu_read_lock();
+	flow = ovs_accel_flow_find(dp_inst, key);
+	if (unlikely(!flow))
+		goto done;
+
+	sf_acts = rcu_dereference(flow->sf_acts);
+	for (a = sf_acts->actions, rem = sf_acts->actions_len; rem > 0;
+			     a = nla_next(a, &rem)) {
+		struct vport *vport;
+		int port_no;
+
+		switch (nla_type(a)) {
+		case OVS_ACTION_ATTR_OUTPUT:
+			port_no = nla_get_u32(a);
+			vport = ovs_vport_ovsl_rcu(dp, port_no);
+			if (!vport) {
+				goto done;
+			}
+
+			dev = vport->dev;
+			rcu_read_unlock();
+			return dev;
+		}
+	}
+done:
+	rcu_read_unlock();
+	return NULL;
+}
+EXPORT_SYMBOL(ovs_accel_egress_dev_find);
+
 static int ovs_vport_cmd_get(struct sk_buff *skb, struct genl_info *info)
 {
 	struct nlattr **a = info->attrs;
diff --git a/net/openvswitch/datapath.h b/net/openvswitch/datapath.h
index 0cd29971a907..2c03ecb84f9a 100644
--- a/net/openvswitch/datapath.h
+++ b/net/openvswitch/datapath.h
@@ -174,6 +174,37 @@ enum ovs_pkt_hash_types {
 	OVS_PACKET_HASH_L4_BIT = (1ULL << 33),
 };
 
+/**
+ *	struct ovs_accel_callback - OVS acceleration callbacks
+ *	@ovs_accel_dp_add - new data path is created
+ *	@ovs_accel_dp_del - data path is deleted
+ *	@ovs_accel_dp_port_add - new port is added into data path
+ *	@ovs_accel_dp_port_del - port is deleted from data path
+ *	@ovs_accel_dp_flow_add - new flow rule is added in data path
+ *	@ovs_accel_dp_flow_del - flow rule is deleted from data path
+ *	@ovs_accel_dp_flow_set - existing flow rule is modified in data path
+ *	@ovs_accel_dp_flow_tbl_flush - flow table is flushed in data path
+ *	@ovs_accel_dp_pkt_process - Process data path packet
+ */
+struct ovs_accel_callback {
+	void (*ovs_accel_dp_add)(void *dp, struct net_device *dev);
+	void (*ovs_accel_dp_del)(void *dp, struct net_device *dev);
+	void (*ovs_accel_dp_port_add)(void *dp, void *vp,
+				      int vp_num, enum ovs_vport_type vp_type,
+				      const char *master, struct net_device *dev);
+	void (*ovs_accel_dp_port_del)(void *dp,  void *vp,
+				      struct net_device *dev);
+	void (*ovs_accel_dp_flow_add)(void *dp, struct sw_flow *sf);
+	void (*ovs_accel_dp_flow_del)(void *dp, struct sw_flow *sf);
+	void (*ovs_accel_dp_flow_set)(void *dp, struct sw_flow *sf,
+				      struct sw_flow_actions *sfa);
+	void (*ovs_accel_dp_flow_tbl_flush)(void *dp);
+	void (*ovs_accel_dp_pkt_process)(void *dp, struct sk_buff *skb,
+					 struct sw_flow_key *key,
+					 struct sw_flow *sf,
+					 struct sw_flow_actions *sfa);
+};
+
 extern unsigned int ovs_net_id;
 void ovs_lock(void);
 void ovs_unlock(void);
@@ -273,6 +304,16 @@ void ovs_dp_notify_wq(struct work_struct *work);
 int action_fifos_init(void);
 void action_fifos_exit(void);
 
+int ovs_register_accelerator(struct ovs_accel_callback *oac);
+void ovs_unregister_accelerator(struct ovs_accel_callback *oac);
+int ovs_accel_flow_stats_update(void *dp, void *out_vport,
+				struct sw_flow_key *sf, int pkts, int bytes);
+struct sw_flow *ovs_accel_flow_find(void *dp, struct sw_flow_key *sfk);
+struct net_device *ovs_accel_dev_find(void *dp, int vport_no);
+struct net_device *ovs_accel_egress_dev_find(void *dp_inst,
+					     struct sw_flow_key *key,
+					     struct sk_buff *skb);
+
 /* 'KEY' must not have any bits set outside of the 'MASK' */
 #define OVS_MASKED(OLD, KEY, MASK) ((KEY) | ((OLD) & ~(MASK)))
 #define OVS_SET_MASKED(OLD, KEY, MASK) ((OLD) = OVS_MASKED(OLD, KEY, MASK))
-- 
2.34.1

